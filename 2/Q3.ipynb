{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Design a feature selection algorithm to find the best features for classifying the Mnist dataset. Implement a bidirectional search algorithm using the provided objective function as the measure for your search algorithm.\n",
    "\n",
    "Use the first 10000 samples of training set in the Mnist dataset for feature selection and training set for kNN approach. Use Euclidean distance to calculate Inter-class.\n",
    "\n",
    "The objective function should be based on this equestion:\n",
    "\n",
    "### J = Inter Class distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "image_size = 28\n",
    "training_samples = 60000\n",
    "\n",
    "# Importing Train Data\n",
    "f_train = gzip.open('train-images-idx3-ubyte.gz','r')\n",
    "f_train.read(16)\n",
    "buf = f_train.read(image_size * image_size * training_samples)\n",
    "train_data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "# train_data = data.reshape(num_images, image_size, image_size)\n",
    "train_data = train_data.reshape(training_samples, image_size* image_size)\n",
    "\n",
    "\n",
    "# Importing Train Labels\n",
    "f_train_label = gzip.open('train-labels-idx1-ubyte.gz','r')\n",
    "f_train_label.read(8)\n",
    "buf = f_train_label.read(training_samples)\n",
    "train_labels = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "\n",
    "\n",
    "# Importing Test Data\n",
    "testing_images = 10000\n",
    "f_test = gzip.open('t10k-images-idx3-ubyte.gz','r')\n",
    "f_test.read(16)\n",
    "buf = f_test.read(image_size * image_size * testing_images)\n",
    "test_data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "test_data = test_data.reshape(testing_images, image_size * image_size)\n",
    "\n",
    "# Importing Test Labels\n",
    "f_test_label = gzip.open('t10k-labels-idx1-ubyte.gz','r')\n",
    "f_test_label.read(8)\n",
    "buf = f_test_label.read(testing_images)\n",
    "test_labels = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "\n",
    "\n",
    "train_data = train_data[0:10000]\n",
    "train_labels = train_labels[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def apply_knn(train_data, train_labels, k):\n",
    "    \n",
    "    train_d =train_data[:8000]\n",
    "    train_l =train_labels[:8000]\n",
    "    \n",
    "    test_d = train_data[8001:]\n",
    "    test_l =train_labels[8001:]\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(train_d,train_l)\n",
    "    \n",
    "    correct = 0\n",
    "    for i in range(len(test_d)):\n",
    "        if model.predict([test_d[i]]) == test_l[i]: correct+=1\n",
    "    \n",
    "    return correct/len(test_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Select the set of {10, 50, 150, 392} features based on the implemented feature selection approach and report the accuracy on the test set of MNIST based on kNN with k = 3. Note: you can take advantage of data structure tricks to speed up the efficiency of kNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 392)\n"
     ]
    }
   ],
   "source": [
    "my_train_data = train_data\n",
    "new_train_data = None\n",
    "\n",
    "for _ in range(392):\n",
    " \n",
    "    # SFS\n",
    "    main_mean = np.mean(my_train_data, axis=0)\n",
    "    mean_i = []\n",
    "    for i in np.unique(train_labels):\n",
    "        indices_of_i = np.where(train_labels == i)[0]\n",
    "        c_i = np.take(my_train_data, indices_of_i, axis=0)\n",
    "        mean_i.append(np.mean(c_i, axis=0))    \n",
    "\n",
    "    total_no_of_features= len(mean_i[0])\n",
    "    J = np.zeros(total_no_of_features)\n",
    "\n",
    "    for i in range(total_no_of_features):\n",
    "        for mean in mean_i:\n",
    "            distance = euclidean(main_mean[i],mean[i])\n",
    "            J[i]+= distance\n",
    "\n",
    "    if new_train_data is None:\n",
    "        new_train_data=my_train_data[:,np.argmax(J)]\n",
    "    else:  \n",
    "        new_train_data = np.c_[new_train_data, my_train_data[:,np.argmax(J)]]\n",
    "\n",
    "    my_train_data = np.delete(my_train_data,np.argmax(J), 1)\n",
    "\n",
    "    # SBS\n",
    "    main_mean = np.mean(my_train_data, axis=0)\n",
    "    mean_i = []\n",
    "    for i in np.unique(train_labels):\n",
    "        indices_of_i = np.where(train_labels == i)[0]\n",
    "        c_i = np.take(my_train_data, indices_of_i, axis=0)\n",
    "        mean_i.append(np.mean(c_i, axis=0))    \n",
    "\n",
    "    total_no_of_features= len(mean_i[0])\n",
    "    J = np.zeros(total_no_of_features)\n",
    "\n",
    "    for i in range(total_no_of_features):\n",
    "        for mean in mean_i:\n",
    "            distance = euclidean(main_mean[i],mean[i])\n",
    "            J[i]+= distance\n",
    "            \n",
    "    my_train_data = np.delete(my_train_data,np.argmin(J), 1)\n",
    "\n",
    "print(new_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data_10 = new_train_data[:,:10]\n",
    "new_train_data_50 = new_train_data[:,:50]\n",
    "new_train_data_150 = new_train_data[:,:150]\n",
    "new_train_data_392 = new_train_data\n",
    "\n",
    "accuracies = []\n",
    "accuracies.append(apply_knn(new_train_data_10,train_labels, 3))\n",
    "accuracies.append(apply_knn(new_train_data_50,train_labels, 3))\n",
    "accuracies.append(apply_knn(new_train_data_150,train_labels, 3))\n",
    "accuracies.append(apply_knn(new_train_data_392,train_labels, 3))\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Visualize the selected features for each set in {10, 50, 150, 392} by a zero 2-D plane where the selected features are pixels set to a value of 1. Compare the 4 different planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Apply LDA on the dataset and report the accuracy based on kNN with k =3. Compare the achieved accuracy by the reported accuracies in part (a). Note: you need to implement LDA method by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9349674837418709\n"
     ]
    }
   ],
   "source": [
    "apply_knn(train_data,train_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
