{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Design a feature selection algorithm to find the best features for classifying the Mnist dataset. Implement a bidirectional search algorithm using the provided objective function as the measure for your search algorithm.\n",
    "\n",
    "Use the first 10000 samples of training set in the Mnist dataset for feature selection and training set for kNN approach. Use Euclidean distance to calculate Inter-class.\n",
    "\n",
    "The objective function should be based on this equestion:\n",
    "\n",
    "### J = Inter Class distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "image_size = 28\n",
    "training_samples = 60000\n",
    "\n",
    "# Importing Train Data\n",
    "f_train = gzip.open('train-images-idx3-ubyte.gz','r')\n",
    "f_train.read(16)\n",
    "buf = f_train.read(image_size * image_size * training_samples)\n",
    "train_data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "# train_data = data.reshape(num_images, image_size, image_size)\n",
    "train_data = train_data.reshape(training_samples, image_size* image_size)\n",
    "\n",
    "\n",
    "# Importing Train Labels\n",
    "f_train_label = gzip.open('train-labels-idx1-ubyte.gz','r')\n",
    "f_train_label.read(8)\n",
    "buf = f_train_label.read(training_samples)\n",
    "train_labels = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "\n",
    "\n",
    "# Importing Test Data\n",
    "testing_images = 10000\n",
    "f_test = gzip.open('t10k-images-idx3-ubyte.gz','r')\n",
    "f_test.read(16)\n",
    "buf = f_test.read(image_size * image_size * testing_images)\n",
    "test_data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "test_data = test_data.reshape(testing_images, image_size * image_size)\n",
    "\n",
    "# Importing Test Labels\n",
    "f_test_label = gzip.open('t10k-labels-idx1-ubyte.gz','r')\n",
    "f_test_label.read(8)\n",
    "buf = f_test_label.read(testing_images)\n",
    "test_labels = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "\n",
    "\n",
    "train_data = train_data[0:10000]\n",
    "train_labels = train_labels[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def apply_knn(train_data, train_labels, k):\n",
    "    \n",
    "    train_d =train_data[:8000]\n",
    "    train_l =train_labels[:8000]\n",
    "    \n",
    "    test_d = train_data[8001:]\n",
    "    test_l =train_labels[8001:]\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(train_d,train_l)\n",
    "    \n",
    "    correct = 0\n",
    "    for i in range(len(test_d)):\n",
    "        if model.predict([test_d[i]]) == test_l[i]: correct+=1\n",
    "    \n",
    "    return correct/len(test_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Select the set of {10, 50, 150, 392} features based on the implemented feature selection approach and report the accuracy on the test set of MNIST based on kNN with k = 3. Note: you can take advantage of data structure tricks to speed up the efficiency of kNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461\n",
      "377\n",
      "592\n",
      "347\n",
      "482\n",
      "587\n",
      "533\n",
      "504\n",
      "477\n",
      "526\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "my_train_data = train_data\n",
    "new_train_data = None\n",
    "\n",
    "for _ in range(10):\n",
    " \n",
    "    # SFS\n",
    "    main_mean = np.mean(my_train_data, axis=0)\n",
    "    mean_i = []\n",
    "    for i in np.unique(train_labels):\n",
    "        indices_of_i = np.where(train_labels == i)[0]\n",
    "        c_i = np.take(my_train_data, indices_of_i, axis=0)\n",
    "        mean_i.append(np.mean(c_i, axis=0))    \n",
    "\n",
    "    total_no_of_features= len(mean_i[0])\n",
    "    J = np.zeros(total_no_of_features)\n",
    "\n",
    "    for i in range(total_no_of_features):\n",
    "        for mean in mean_i:\n",
    "            distance = euclidean(main_mean[i],mean[i])\n",
    "            J[i]+= distance\n",
    "    \n",
    "    print(np.argmax(J))\n",
    "    if new_train_data is None:\n",
    "        new_train_data=my_train_data[:,np.argmax(J)]\n",
    "    else:  \n",
    "        new_train_data = np.c_[new_train_data, my_train_data[:,np.argmax(J)]]\n",
    "    my_train_data = np.delete(my_train_data,np.argmax(J), 1)\n",
    "\n",
    "    # SBS\n",
    "    main_mean = np.mean(my_train_data, axis=0)\n",
    "    mean_i = []\n",
    "    for i in np.unique(train_labels):\n",
    "        indices_of_i = np.where(train_labels == i)[0]\n",
    "        c_i = np.take(my_train_data, indices_of_i, axis=0)\n",
    "        mean_i.append(np.mean(c_i, axis=0))    \n",
    "\n",
    "    total_no_of_features= len(mean_i[0])\n",
    "    J = np.zeros(total_no_of_features)\n",
    "\n",
    "    for i in range(total_no_of_features):\n",
    "        for mean in mean_i:\n",
    "            distance = euclidean(main_mean[i],mean[i])\n",
    "            J[i]+= distance\n",
    "    my_train_data = np.delete(my_train_data,np.argmin(J), 1)\n",
    "\n",
    "print(new_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "my_train_data = train_data\n",
    "new_train_data = None\n",
    "\n",
    "\n",
    "deleted_j_max_indices = []\n",
    "deleted_j_min_indices = []\n",
    "for _ in range(10):\n",
    "    \n",
    "    # SFS\n",
    "    main_mean = np.mean(my_train_data, axis=0)\n",
    "    mean_i = []\n",
    "    for i in np.unique(train_labels):\n",
    "        indices_of_i = np.where(train_labels == i)[0]\n",
    "        c_i = np.take(my_train_data, indices_of_i, axis=0)\n",
    "        mean_i.append(np.mean(c_i, axis=0))    \n",
    "\n",
    "    J_max = 0\n",
    "    j_max_index = 0\n",
    "    for i in range(784):\n",
    "        if i not in deleted_j_max_indices and i not in deleted_j_min_indices:\n",
    "            distance = 0\n",
    "            for mean in mean_i:\n",
    "                distance += euclidean(main_mean[i],mean[i])\n",
    "            if distance >= J_max:\n",
    "                J_max = distance\n",
    "                j_max_index = i\n",
    "   \n",
    "    deleted_j_max_indices.append(j_max_index)\n",
    "    \n",
    "    if new_train_data is None:\n",
    "        new_train_data=my_train_data[:,j_max_index]\n",
    "    else:  \n",
    "        new_train_data = np.c_[new_train_data, my_train_data[:,j_max_index]]\n",
    "\n",
    "    \n",
    "    # SBS\n",
    "    main_mean = np.mean(my_train_data, axis=0)\n",
    "    mean_i = []\n",
    "    for i in np.unique(train_labels):\n",
    "        indices_of_i = np.where(train_labels == i)[0]\n",
    "        c_i = np.take(my_train_data, indices_of_i, axis=0)\n",
    "        mean_i.append(np.mean(c_i, axis=0))  \n",
    "        \n",
    "    J_min = J_max\n",
    "    j_min_index = 0\n",
    "    for i in range(784):\n",
    "        if i not in deleted_j_max_indices and i not in deleted_j_min_indices:\n",
    "            distance = 0\n",
    "            for mean in mean_i:\n",
    "                distance += euclidean(main_mean[i],mean[i])\n",
    "            if distance <= J_min:\n",
    "                J_min = distance\n",
    "                j_min_index = i\n",
    "    \n",
    "    deleted_j_min_indices.append(j_min_index)\n",
    "\n",
    "print(new_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5812906453226613"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_knn(new_train_data,train_labels, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[461, 378, 596, 350, 489, 597, 543, 515, 488, 541]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleted_j_max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5812906453226613, 0.8164082041020511, 0.9134567283641821, 0.9364682341170585]\n"
     ]
    }
   ],
   "source": [
    "new_train_data_10 = new_train_data[:,:10]\n",
    "new_train_data_50 = new_train_data[:,:50]\n",
    "new_train_data_150 = new_train_data[:,:150]\n",
    "new_train_data_392 = new_train_data\n",
    "\n",
    "accuracies = []\n",
    "accuracies.append(apply_knn(new_train_data_10,train_labels, 3))\n",
    "accuracies.append(apply_knn(new_train_data_50,train_labels, 3))\n",
    "accuracies.append(apply_knn(new_train_data_150,train_labels, 3))\n",
    "accuracies.append(apply_knn(new_train_data_392,train_labels, 3))\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Visualize the selected features for each set in {10, 50, 150, 392} by a zero 2-D plane where the selected features are pixels set to a value of 1. Compare the 4 different planes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.zeros(784)\n",
    "for val in deleted_j_max_indices:\n",
    "    data[val] = 1.0\n",
    "data = data.reshape((28, 28))\n",
    "plt.imshow(data);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Apply LDA on the dataset and report the accuracy based on kNN with k =3. Compare the achieved accuracy by the reported accuracies in part (a). Note: you need to implement LDA method by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_train_data = train_data\n",
    "\n",
    "overall_mean = np.mean(my_train_data, axis=0)\n",
    "means = []\n",
    "for i in np.unique(train_labels):\n",
    "    indices_of_i = np.where(train_labels == i)[0]\n",
    "    c_i = np.take(my_train_data, indices_of_i, axis=0)\n",
    "    mean_i.append(np.mean(c_i, axis=0))   \n",
    "\n",
    "S_W = np.zeros((len(my_train_data),len(my_train_data[0])))\n",
    "\n",
    "\n",
    "for m in means:\n",
    "    \n",
    "S_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-307-6d65b625de28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mestimate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-307-6d65b625de28>\u001b[0m in \u001b[0;36mestimate_params\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# S_B = \\sigma{N_i (m_i - m) (m_i - m).T}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mS_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             S_B += np.multiply(len(self.classwise[c]),\n\u001b[1;32m     27\u001b[0m                                np.outer((means[c] - overall_mean), \n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "def estimate_params(data):\n",
    "        # group data by label column\n",
    "#         grouped = data.groupby(self.data.ix[:,self.labelcol])\n",
    "\n",
    "        # calculate means for each class\n",
    "#         means = {}\n",
    "#         for c in self.classes:\n",
    "#             means[c] = np.array(self.drop_col(self.classwise[c], self.labelcol).mean(axis = 0))\n",
    "\n",
    "        # calculate the overall mean of all the data\n",
    "        overall_mean = np.array(self.drop_col(data, self.labelcol).mean(axis = 0))\n",
    "\n",
    "        \n",
    "        overall_mean = np.mean(my_train_data, axis=0)\n",
    "        means = []\n",
    "        for i in np.unique(train_labels):\n",
    "            indices_of_i = np.where(train_labels == i)[0]\n",
    "            c_i = np.take(my_train_data, indices_of_i, axis=0)\n",
    "            mean_i.append(np.mean(c_i, axis=0))    \n",
    "\n",
    "        \n",
    "        # calculate between class covariance matrix\n",
    "        # S_B = \\sigma{N_i (m_i - m) (m_i - m).T}\n",
    "        S_B = np.zeros((data.shape[1] - 1, data.shape[1] - 1))\n",
    "        for c in means.keys():\n",
    "            S_B += np.multiply(len(self.classwise[c])\n",
    "                               , np.outer((means[c] - overall_mean)\n",
    "                                          , (means[c] - overall_mean)))\n",
    "\n",
    "        # calculate within class covariance matrix\n",
    "        # S_W = \\sigma{S_i}\n",
    "        # S_i = \\sigma{(x - m_i) (x - m_i).T}\n",
    "        S_W = np.zeros(S_B.shape) \n",
    "        for c in self.classes: \n",
    "            tmp = np.subtract(self.drop_col(self.classwise[c], self.labelcol).T, np.expand_dims(means[c], axis=1))\n",
    "            S_W = np.add(np.dot(tmp, tmp.T), S_W)\n",
    "\n",
    "        # objective : find eigenvalue, eigenvector pairs for inv(S_W).S_B\n",
    "        mat = np.dot(np.linalg.pinv(S_W), S_B)\n",
    "        eigvals, eigvecs = np.linalg.eig(mat)\n",
    "        eiglist = [(eigvals[i], eigvecs[:, i]) for i in range(len(eigvals))]\n",
    "\n",
    "        # sort the eigvals in decreasing order\n",
    "        eiglist = sorted(eiglist, key = lambda x : x[0], reverse = True)\n",
    "\n",
    "        # take the first num_dims eigvectors\n",
    "        w = np.array([eiglist[i][1] for i in range(self.num_dims)])\n",
    "\n",
    "        self.w = w\n",
    "        self.means = means\n",
    "        print(w.shape)\n",
    "        return\n",
    "    \n",
    "print(train_data.shape)\n",
    "estimate_params(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from sklearn.model_selection import train_test_split\n",
    "style.use('fivethirtyeight')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# 0. Load in the data and split the descriptive and the target feature\n",
    "df = pd.read_csv('data/Wine.txt',sep=',',names=['target','Alcohol','Malic_acid','Ash','Akcakinity','Magnesium','Total_pheonols','Flavanoids','Nonflavanoids','Proanthocyanins','Color_intensity','Hue','OD280','Proline'])\n",
    "X = df.iloc[:,1:].copy()\n",
    "target = df['target'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,target,test_size=0.3,random_state=0) \n",
    "# 1. Standardize the data\n",
    "for col in X_train.columns:\n",
    "    X_train[col] = StandardScaler().fit_transform(X_train[col].values.reshape(-1,1))\n",
    "# 2. Compute the mean vector mu and the mean vector per class mu_k\n",
    "mu = np.mean(X_train,axis=0).values.reshape(13,1) # Mean vector mu --> Since the data has been standardized, the data means are zero \n",
    "mu_k = []\n",
    "for i,orchid in enumerate(np.unique(df['target'])):\n",
    "    mu_k.append(np.mean(X_train.where(df['target']==orchid),axis=0))\n",
    "mu_k = np.array(mu_k).T\n",
    "# 3. Compute the Scatter within and Scatter between matrices\n",
    "data_SW = []\n",
    "Nc = []\n",
    "for i,orchid in enumerate(np.unique(df['target'])):\n",
    "    a = np.array(X_train.where(df['target']==orchid).dropna().values-mu_k[:,i].reshape(1,13))\n",
    "    data_SW.append(np.dot(a.T,a))\n",
    "    Nc.append(np.sum(df['target']==orchid))\n",
    "SW = np.sum(data_SW,axis=0)\n",
    "SB = np.dot(Nc*np.array(mu_k-mu),np.array(mu_k-mu).T)\n",
    "   \n",
    "# 4. Compute the Eigenvalues and Eigenvectors of SW^-1 SB\n",
    "eigval, eigvec = np.linalg.eig(np.dot(np.linalg.inv(SW),SB))\n",
    "    \n",
    "# 5. Select the two largest eigenvalues \n",
    "eigen_pairs = [[np.abs(eigval[i]),eigvec[:,i]] for i in range(len(eigval))]\n",
    "eigen_pairs = sorted(eigen_pairs,key=lambda k: k[0],reverse=True)\n",
    "w = np.hstack((eigen_pairs[0][1][:,np.newaxis].real,eigen_pairs[1][1][:,np.newaxis].real)) # Select two largest\n",
    "# 6. Transform the data with Y=X*w\n",
    "Y = X_train.dot(w)\n",
    "# Plot the data\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax0 = fig.add_subplot(111)\n",
    "ax0.set_xlim(-3,3)\n",
    "ax0.set_ylim(-4,3)\n",
    "for l,c,m in zip(np.unique(y_train),['r','g','b'],['s','x','o']):\n",
    "    ax0.scatter(Y[0][y_train==l],\n",
    "                Y[1][y_train==l],\n",
    "               c=c, marker=m, label=l,edgecolors='black')\n",
    "ax0.legend(loc='upper right')\n",
    "# Plot the voroni spaces\n",
    "means = []\n",
    "for m,target in zip(['s','x','o'],np.unique(y_train)):\n",
    "    means.append(np.mean(Y[y_train==target],axis=0))\n",
    "    ax0.scatter(np.mean(Y[y_train==target],axis=0)[0],np.mean(Y[y_train==target],axis=0)[1],marker=m,c='black',s=100)\n",
    "   \n",
    "mesh_x, mesh_y = np.meshgrid(np.linspace(-3,3),np.linspace(-4,3)) \n",
    "mesh = []\n",
    "for i in range(len(mesh_x)):\n",
    "    for j in range(len(mesh_x[0])):\n",
    "        date = [mesh_x[i][j],mesh_y[i][j]]\n",
    "        mesh.append((mesh_x[i][j],mesh_y[i][j]))\n",
    "NN = KNeighborsClassifier(n_neighbors=1)\n",
    "NN.fit(means,['r','g','b'])        \n",
    "predictions = NN.predict(np.array(mesh))\n",
    "ax0.scatter(np.array(mesh)[:,0],np.array(mesh)[:,1],color=predictions,alpha=0.3)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9349674837418709\n"
     ]
    }
   ],
   "source": [
    "apply_knn(train_data,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3], [4]]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
